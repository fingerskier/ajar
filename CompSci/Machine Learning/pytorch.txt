import torch

a computation is an acyclic graph of Tensors and Functions
- encodes the computational history

torch.Tensor
    .requires_grad = True ~> tracks all operations for purposes of backpropogation
    ...defaults to False
    .backward() computes gradients and stores them in .grad
    .detach() stops tracking operation history
    ||
    with torch.no_grad():
        # stuff in here is not tracked

torch.Function
    <tensor>.grad_fn ~> Function that created the Tensor
        ...Tensors created by the dev have .grad_fn=None
    T.backward(T) ~ where T is a Tensor of matching shape
    ...unless the Tensor holds only one element
